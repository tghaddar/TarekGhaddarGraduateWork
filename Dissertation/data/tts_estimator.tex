%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  New template code for TAMU Theses and Dissertations starting Fall 2016.  
%
%
%  Author: Sean Zachary Roberson
%  Version 3.17.09
%  Last Updated: 9/21/2017
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           TIME TO SOLUTION ESTIMATOR CHAPTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \setcounter{MaxMatrixCols}{30}
\newcommand{\tcr}[1]{\textcolor{red}{#1}}


\chapter{TIME-TO-SOLUTION ESTIMATOR \label{cha:tts}}

Before optimization of the partitioning scheme can occur, it is necessary to have an estimation tool that gives the approximate sweep time for a given partitioning scheme. The time-to-solution estimator serves as the objective function that gets optimized, with the partitions serving as the parameter space. This chapter will detail the time-to-solution estimator, and showcase the results of 2D and 3D verification studies. 

The time to solution estimator is written in Python 3. Python was chosen as the language because of its powerful graph library, networkx. 
This library gives us a wide variety of graph mathematics and is easy to use. 
Before detailing the time-to-solution estimator's methodology, a description of the applicable basic graph theory is necessary. 

\section{Graph Theory Applicable to the Time-to-Solution Estimator}

Graph theory is a subset of mathematics that focuses on graphs, or structures containing a set of objects that are connected in a particular manner \cite{graphtheory}. 
These objects are called vertices (or nodes), and are connected by edges. 
Figure \ref{basic_graph} shows an undirected graph of 4 nodes and 4 edges. The nodes are a mathematical abstraction that can be used to represent a variety of concepts. In this dissertation, they are used to represent the subsets described in Chapter \ref{cha:lb}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{../../figures/undirected_graph.pdf}
\caption{An undirected graph with 4 nodes and 4 edges. }
\label{basic_graph}
\end{figure}
Figure \ref{basic_graph} is referred to as an undirected because its edges have no directional information. 
If we add directional information to the edges of the graph in Fig. \ref{basic_graph}, it becomes a directed graph, shown in Fig. \ref{directed_graph}. 
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{../../figures/directed_graph.pdf}
\caption{A directed graph with 4 nodes and 4 edges. }
\label{directed_graph}
\end{figure}
In this dissertation, we only use Directed Acyclic Graphs (DAGs), or a graph that has no cycles.  A cycle is defined as a path on a graph that starts and ends at the same vertex. Figure \ref{cycle_example} shows a cycle between nodes 1 and 3. 
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{../../figures/cycle_example.pdf}
\caption{A directed graph with a cycle between nodes 1 and 3.}
\label{cycle_example}
\end{figure}
We notice the edge connecting nodes 1 and 3 has a double headed arrow, representing a cycle. Graphs with cycles can be solved for a variety of applications through the use of cycle detection and breaking algorithms, but our partitioning scheme cuts subsets in a fashion that doesn't allow for cyclical graphs.

Graph edges can be weighted based on the need of the application the graph is being used for. Here, we weight the graph edges to represent the time it takes to solve node A plus the communication time to node B. 
Figure \ref{weighted_directed_graph} shows a weighted directed graph. 
In the context of the time-to-solution estimator, subset 1 takes 3 seconds to solve and communicate to subset 3. 
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{../../figures/weighted_directed_graph.pdf}
\caption{A weighted directed acyclic graph with 4 nodes and 4 weighted edges.}
\label{weighted_directed_graph}
\end{figure}

The time-to-solution estimator uses Johnson's algorithm \cite{intro_to_alg,johnson_nist,johnson_johnson} to find all shortest paths between all pairs in a weighted directed graph. 
The weighted shortest path is defined as the path between two nodes that has the smallest weighted sum. 
For example, the shortest path between nodes 0 and 3 in Fig. \ref{weighted_directed_graph} is $0\rightarrow 2\rightarrow 3$. 

In our application, we use Johnson's algorithm to assist in calculate the longest path between two nodes in a DAG (needed in Section \ref{sec:universal}). 
The longest path is found by:
\begin{enumerate}
  \item Multiplying all edge weights by -1,
  \item Using Johnson's algorithm to find the shortest (in this case the most negative) paths,
  \item Summing the original weights of this shortest path. 
\end{enumerate}
Johnson's algorithm is specifically used in this process because it is capable of finding the shortest path even when edge weights are negative. 

%In addition to longest path length calculation, we use an unweighted shortest path algorithm when calculating the depth-of-graph remaining (needed in Section \ref{sec:conflict}). 

%\begin{figure}[H]
%\centering
%\includegraphics[scale=0.5]{../../figures/negative_weighted_directed_graph.pdf}
%\caption{The DAG in Fig. \ref{
%\label{negative_weights}
%\end{figure}
Now that the applicable graph theory has been reviewed, we detail the methodology of the time-to-solution estimator.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
The time-to-solution estimator determines the time to sweep across a domain by:
\begin{enumerate}
	\item Given a partitioning scheme, building an adjacency matrix,
	\item Building Directed Acyclic Graphs (DAGs) from the adjacency matrix, one for each quadrant/octant,
	\item Weighting the edges of each graph based on the solve time and communication time of each subset to its neighbors,
     \item Adding and modifying graph weights based on how many anglesets are pipelined,
	\item Modifying the weights of each graph to operate on the universal timescale,
	\item Modifying the weights of each graph to reflect sweep conflicts between octants, 
	\item Calculating the time to solution.
\end{enumerate}

\subsection{Building the adjacency matrices}

Before building the graph for each quadrant/octant, an adjacency matrix must be built for the given partitioning scheme. 
The adjacency matrix proves connectivity information for each subset to its neighboring subset. 
The adjacency building process relies on a major assumption: The z dimension has partitions all the way across the domain, then the x dimension has partitions per plane, then the y dimension has partitions per plane per column. 
In future work, these three dimensions will be interchangeable, but for now, this ordering must be preserved. 

Figure \ref{25basematrix} shows a partitioning scheme for 3 subsets each in the x and y dimensions with its corresponding adjacency matrix.

\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\centering
\includegraphics[scale=0.55]{../../figures/boundaries_worst.pdf}
\end{minipage}
\begin{minipage}[c]{0.6\textwidth}
\centering
\scalebox{0.5}{
$\begin{pmatrix}
0&1&0&1&0&0&0&0&0\\ 
1&0&1&1&0&0&0&0&0\\ 
0&1&0&1&1&1&0&0&0\\ 
1&1&1&0&1&0&1&1&1\\ 
0&0&1&1&0&1&0&0&1\\ 
0&0&1&0&1&0&0&0&1\\ 
0&0&0&1&0&0&0&1&0\\ 
0&0&0&1&0&0&1&0&1\\ 
0&0&0&1&1&1&0&1&0\\ 
\end{pmatrix}$}
\end{minipage}
\caption{A 3x3 subset partitioning scheme and its corresponding adjacency matrix.}
\label{25basematrix}
\end{figure}

\subsection{Building the directed acyclic graphs (DAGs)}

The adjacency matrices give us directed connectivity information in order to build our graphs. This process differs slightly from 2D to 3D. Both processes use on networkx's DiGraph function to build the DAGs.

\subsubsection{Building the 2d graphs}
In two dimensions, we build four graphs corresponding to four quadrants. 
We define the quadrants in the following manner:
\begin{itemize}
  \item Quadrant 0: $\Omega_x > 0$, $\Omega_y > 0$
  \item Quadrant 1: $\Omega_x > 0$, $\Omega_y < 0$
  \item Quadrant 2: $\Omega_x < 0$, $\Omega_y > 0$
  \item Quadrant 3: $\Omega_x < 0$, $\Omega_y < 0$
\end{itemize}
Figure \ref{quadrant_layout} illustrates this numbering.
\begin{figure}[H]
\centering
\includegraphics{figures/quadrant_layout.pdf}
\caption{The quadrant layout for 2D problems.}
\label{quadrant_layout}
\end{figure}
The initial adjacency matrix we obtain can be immediately used to build the graphs for quadrants 0 and 3 by using networkx's DiGraph function.
We feed the upper triangular portion of the adjacency matrix to DiGraph to get the quadrant 0 graph, and the lower triangular portion to get the quadrant 3 graph.
Utilizing the same partitioning scheme shown in Fig. \ref{25basematrix}, pull the upper triangular and lower triangular portions of the matrix, shown in Fig. \ref{25baseportionmatrices}.
\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\centering
\scalebox{0.6}{
$\begin{pmatrix}
0&1&0&1&0&0&0&0&0\\ 
0&0&1&1&0&0&0&0&0\\ 
0&0&0&1&1&1&0&0&0\\ 
0&0&0&0&1&0&1&1&1\\ 
0&0&0&0&0&1&0&0&1\\ 
0&0&0&0&0&0&0&0&1\\ 
0&0&0&0&0&0&0&1&0\\ 
0&0&0&0&0&0&0&0&1\\ 
0&0&0&0&0&0&0&0&0\\ 
\end{pmatrix}$}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
\centering
\scalebox{0.6}{
$\begin{pmatrix}
0&0&0&0&0&0&0&0&0\\ 
1&0&0&0&0&0&0&0&0\\ 
0&1&0&0&0&0&0&0&0\\ 
1&1&1&0&0&0&0&0&0\\ 
0&0&1&1&0&0&0&0&0\\ 
0&0&1&0&1&0&0&0&0\\ 
0&0&0&1&0&0&0&0&0\\ 
0&0&0&1&0&0&1&0&0\\ 
0&0&0&1&1&1&0&1&0\\ 
\end{pmatrix}$}
\end{minipage}
\caption{The upper triangular (left) and lower triangular (right) portions of the adjacency matrix in Fig. \ref{25basematrix}.}
\label{25baseportionmatrices}
\end{figure}
These matrix portions provide connectivity \textit{and} dependency information for quadrants 0 and 3. 
With this information, network's DiGraph function is able to construct the DAGs for these quadrants. 
Figure \ref{25_q0q3graphs} shows the the DAGS associated with quadrants 0 and 3, built from the adjacency matrices in Fig. \ref{25baseportionmatrices}.

\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\includegraphics[scale=0.5]{../../figures/9_graph0.pdf}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
\includegraphics[scale=0.5]{../../figures/9_graph3.pdf}
\end{minipage}
\caption{The quadrant 0 DAG (left) and the quadrant 3 DAG(right).}
\label{25_q0q3graphs}
\end{figure}
Upon inspection of these two DAGs, we see that they show the correct connectivity, dependency, and expected opposing sweep ordering. 
Quadrant 0 starts its sweep from subset 0, finishing at subset 8, and quadrant 3 starts its sweep from subset 8, finishing at subset 0.

To obtain the graphs for quadrants 1 and 2, a ``flipped'' version of the adjacency matrix is necessary. 
We temporarily renumber the subsets starting from the top left corner, and increasing down each column, as shown in Fig. \ref{25flippedmatrix}.
\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\centering
\includegraphics[scale=0.55]{../../figures/boundaries_worst_flipped.pdf}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
\centering
\scalebox{0.6}{
$\begin{pmatrix}
0&1&0&1&1&1&0&0&0\\ 
1&0&1&0&0&1&0&0&0\\ 
0&1&0&0&0&1&0&0&0\\ 
1&0&0&0&1&0&1&0&0\\ 
1&0&0&1&0&1&1&0&0\\ 
1&1&1&0&1&0&1&1&1\\ 
0&0&0&1&1&1&0&1&0\\ 
0&0&0&0&0&1&1&0&1\\ 
0&0&0&0&0&1&0&1&0\\ 
\end{pmatrix}$}
\end{minipage}
\caption{The flipped subset ordering and corresponding flipped adjacency matrix for the partitioning scheme in Fig. \ref{25basematrix}.}
\label{25flippedmatrix}
\end{figure}
We then get the upper triangular and lower triangular portions of the flipped adjacency matrix to get the connectivity and dependency information for quadrants 1 and 2. 
We feed the triangular matrices into networkx's DiGraph function, along with a mapping of the flipped subset ids to the original subset ids (shown in Fig. \ref{25basematrix}). Figure \ref{25_q1q2graphs} shows the DAGs for quadrants 1 and 2.
\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\includegraphics[scale=0.5]{../../figures/9_graph1.pdf}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
\includegraphics[scale=0.5]{../../figures/9_graph2.pdf}
\end{minipage}
\caption{The quadrant 1 DAG (left) and the quadrant 2 DAG(right).}
\label{25_q1q2graphs}
\end{figure}
Upon inspection of these two DAGs, we see that they show the correct connectivity, dependency, and expected opposing sweep ordering. 
Quadrant 1 starts its sweep from subset 2, finishing at subset 6, and quadrant 2 starts its sweep from subset 6, finishing at subset 2.

%Weighting the TDGS
\subsection{Weighting the task dependence graphs}

Each graph is weighted to reflect the solve time and communication time of each node to its neighbors. Explicitly, the edge weight between node A and node B represents the solve time of node A added to the time it takes to communicate boundary information from nod A to node B. Equation \ref{weight_function} shows how the weights are calculated:
\begin{equation}
\text{weight} = \text{mcff}\cdot [T_{wu} + N_n\cdot \text{latency}\cdot M_L + T_{\text{comm}}\cdot N_b\cdot A_m\cdot upbc + \cdot N_c\cdot (T_c + A_m\cdot (T_m + T_g))],
\label{weight_function}
\end{equation}
where:
\begin{itemize}
  \item mcff = the Multi-Core Fudge Factor, a corrective factor that accounts for performance drop-off from 1 to 8 cores,
  \item $T_{wu}$ = the time to get into the sweep operator,
  \item $N_n$ = the number of neighbors this node has to communicate to,
  \item latency = the machine specific communication latency,
  \item $M_L$ = the machine specific latency multiplier,
  \item $T_{\text{comm}}$ = the communication time per double,
  \item $N_b$ = the number of boundary cells shared by node A and node B,
  \item $A_m$ = the number of angles node A has to solve prior to communicating,
  \item $upbc$ = the number of boundary unknowns per boundary cell,
  \item $N_c$ = the number of cells in node A,
  \item $T_c$ = the time spent solving cell-specific work,
  \item $T_m$ = the time spent solving angle-specific work,
  \item $T_g$ = the time spent solving group-specific work.
\end{itemize}
The weighting function is based on PDT's performance model\cite{mpadams2015}, which is specific to how PDT solves the transport sweep. The cost function can be modified based on different sweep methodologies if a user desires.

As shown in Eq. \ref{weight_function}, a crucial part of determining the weight of each edge is knowing the number of cells each subset has, and the amount of shared boundary cells with each neighbor. Given a mesh density, the number of cells per subset is given by Eq \ref{cellspersubset}:
\begin{equation}
   \text{cells per subset} = \int_{\tcr{x_i}}^{\tcr{x_{i+1}}} \int_{\tcr{y_j}}^{\tcr{y_{j+1}}} \int_{\tcr{z_k}}^{\tcr{z_{k+1}}} \text{mesh density } dx dy dz,
\label{cellspersubset}
\end{equation}
where the integral bounds represent the cut plane coordinates that form the subset. To estimate the boundary cells to each neighbor, we assume that the mesh within each subset is mostly uniform. Equations \ref{nxy}-\ref{nyz} calculate the boundary cells along each face in 3D:
\begin{align}
n_{xy} &= \Big(\frac{N_c}{V}\Big)^{2/3}\cdot L_x\cdot L_y \label{nxy}, \\
n_{xz} &= \Big(\frac{N_c}{V}\Big)^{2/3}\cdot L_x\cdot L_z \label{nxz}, \\
n_{yz} &= \Big(\frac{N_c}{V}\Big)^{2/3}\cdot L_y\cdot L_z \label{nyz},
\end{align}
where $N_c$ is the number of cells in the subset, $V$ is the subset volume, and $L_d$ is the length of the subset in dimension $d$.
Equations \ref{nx} and \ref{ny} show the 2-dimensional equivalents to \ref{nxy}-\ref{nyz}:
\begin{align}
n_x &= \Big(\frac{N_c}{A}\Big)\cdot L_x, \label{nx} \\
n_y &= \Big(\frac{N_c}{A}\Big)\cdot L_y, \label{ny}
\end{align}
where $A$ is the subset area. With this information, we compute all weights in each graph according to Eq. \ref{weight_function}. Once graphs are weighted, we add and modify edge weights for the number of angles pipelined per octant/quadrant.

\subsection{Adding graphs for angular pipelining}

If there are angles to be pipelined, the time-to-solution estimator adds a new set of graphs for each additional angle to be pipelined. For example, if there are two anglesets per octant, this would results in 16 graphs, or 1 graph per octant per angleset. Figure \ref{angular_pipeline} shows the graphs for two anglesets for a quadrant. A ``dummy'' node with a value of -2 is added in order to have an incoming edge for the first subset's node. The value of this incoming edge represents when that angleset starts its sweep. 
\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\centering
\includegraphics[scale=0.6]{../../figures/q0_postpipeline.pdf}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
\centering
\includegraphics[scale=0.6]{../../figures/q4_postpipeline.pdf}
\end{minipage}
\caption{The graphs for the first (left) and second (right) anglesets}
\label{angular_pipeline}
\end{figure}

\subsection{Modifying the weights of each graph to reflect a universal timescale}\label{sec:universal}

Once we have our full set of graphs with angular pipelining accounted for, we set up each graph to reflect a universal timescale, with the goal of knowing when each node in each graph is ready to solve. 
For each node in each graph, we:
\begin{enumerate}
  \item Calculate the longest path to the node,
  \item Sum the weights of the edges along the longest path,
  \item Set all incoming edge values to the node to the sum of the longest path. 
\end{enumerate}
The incoming edges to each node in each graph now reflect the time at which it is ready to solve. This universal edge weighting is crucial for detecting and resolving conflicts during the sweep. Figure \ref{universal} shows a simple example of a TDG before and after this weight adjustment.
\begin{figure}[H]
  \begin{minipage}[c]{0.5\textwidth}
    \centering
    \includegraphics[scale=0.5]{../../figures/G_pre_universal.pdf}
  \end{minipage}
  \begin{minipage}[c]{0.5\textwidth}
    \centering
    \includegraphics[scale=0.5]{../../figures/G_universal.pdf}
  \end{minipage}
  \caption{A TDG before (left) and after (right) universal edge weighting is applied.}
   \label{universal}
\end{figure}

\subsection{Modifying the weights of each graph to detect and resolve conflicts}\label{sec:conflict}

At this point in the time-to-solution estimation process, we have a graph per octant/quadrant per angleset, each weighted on a universal time scale. The time to solution is best summarized as a ``marching'' process:
\begin{enumerate}
  \item Starting at time $t=0$, we find the first interaction across all graphs. 
  \item If at that time, multiple graphs are solving the same node, we have a conflict. 
  \item The graph that ``wins'' the conflict does not have its weights modified, while the graph that lose the conflict modify their downstream weights according to how long they are delayed. 
  \item Update $t$ to the next interaction across all graphs. 
  \item Repeat steps 3 and 4 until all graphs are finished sweeping.
\end{enumerate}

When a conflict is detected, the time-to-solution estimator uses a first-come-first-serve conflict resolution method. The first graph to arrive to a node will begin solving it, and the remaining graphs that arrive while it is being solved will incur a delay. The delay is reflected in the remaining graphs by adding the delay as a weight to the applicable edge and all downstream edges in the losing graphs. 

If two or more graphs arrive to a node at the same time, the octant with the greater remaining depth-of-graph (simply, more work remaining), wins. In the case of a tie in the depth-of-graph, the graph with the priority direction wins according to the following rules:
\begin{enumerate}
    \item The graph with $\Omega_x > 0$ wins,
	\item If multiple graphs have $\Omega_x > 0$, then the task with $\Omega_y > 0$ wins,
	\item If multiple graphs have $\Omega_y > 0$, then the task with $\Omega_z > 0$ wins.
\end{enumerate}
The delay is once again added to the applicable edge's weight and all downstream edges' weights.

\subsection{Estimating the final time-to-solution}

Once all graphs have had their weights adjusted for conflicts, the graphs now reflect a schedule. The incoming edges to each node in each graph represent what time they are ready to solve. The final weight in each graph represents the time it takes for that graph to sweep across its domain. The maximum final weight across all graphs represents the estimate for the time-to-solution for the problem.

\section{2D Verification}

A verification study in 2D was run to verify the time-to-solution estimator for 2D partitioning schemes with perfectly balanced partitions. The test problems were verified against a code written by Jean Ragusa that mimics PDT's scheduler in two dimensions. For consistency, the time-to-solution estimator utilized an unweighted depth-of-graph algorithm during the verification study to match PDT's scheduling. The verification study consists of the following problems:
\begin{enumerate}
	\item 2x2 to 10x10 subsets in x and y with regular partitions and 1 to 6 angles per quadrant.
	\item 2x2 to 10x10 subsets in x and y with mildly random partitions and 1 to 6 angles per quadrant.
	\item  2x2 to 10x10 subsets in x and y with random partitions and 1 to 6 angles per quadrant.
	\item  2x2 to 10x10 subsets in x and y with probable worst-case partitions and 1 to 6 angles per quadrant.
\end{enumerate}

"Mildly random" partitions keep the cut lines uniformly distributed in x, while the y cut lines vary slightly around the uniformly distributed cut lines of the regular partitions. Figure \ref{mild_random_partitions} shows examples of this partitioning style. "Random" partitions possesses no such limitations on either set of cut lines, as shown by Fig. \ref{random_partitions}. 

Figures \ref{regular_partitions}, \ref{mild_random_partitions}, \ref{random_partitions}, \ref{worst_partitions} show the four partitioning schemes and Figs. \ref{regular_verification}, \ref{mild_random_verification}, \ref{random_verification}, \ref{worst_verification} show the results of the verification study for each partitioning scheme. In the results, a stage is defined as the time it takes to solve all cells in a subset for an angle.  

\subsection{Regular Partitions}

Figure \ref{regular_partitions} shows four examples of the regular partitioning scheme used for the first part of the verification study. Cut lines in both dimensions go all the way across the domain. This reflects the partitioning scheme after the original load balancing algorithm described in Section \ref{sec:og_lb} is used.

%Regular partitions
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/4_regular.pdf}
  \caption{4x4 subsets with regular partitions.}
  \label{4regular}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/6_regular.pdf}
  \caption{6x6 subsets with regular partitions.}
  \label{6regular}
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/8_regular.pdf}
  \caption{8x8 subsets with regular partitions.}
  \label{8regular}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/10_regular.pdf}
  \caption{10x10 subsets with regular partitions.}
  \label{10regular}
\end{subfigure}
\caption{Examples of regular partitioning.}
\label{regular_partitions}
\end{figure}

Using regular partitions as shown in Fig. \ref{regular_partitions}, the first portion of the 2D verification study was run from 2x2 to 10x10 subsets in x and y and 1 to 6 angles per quadrant.  Figure \ref{regular_verification} shows the results of the time-to-solution estimator (solid line) against Ragusa's code (points) for each test case. The time-to-solution estimator verifies perfectly on regular partitions with multiple angles per quadrant. 

%Verification plots.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{../figures/regular_verification.pdf}
\caption{A 2D verification suite with regular partitions run from 2x2 to 10x10 subsets with each case being run from 1 to 6 angles per quadrant.}
\label{regular_verification}
\end{figure}

\subsection{Mildly Random Partitions}
Figure \ref{mild_random_partitions} shows four examples of the mildly random partitioning scheme used for the second part of the verification study. Cut lines in the x dimension go all the way across the domain, and are uniformly distributed. This reflects a possible partitioning scheme after the load balancing by dimension algorithm described in Section \ref{sec:lbd} is used.

%Mild random partitions
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/4_mild_random.pdf}
  \caption{4x4 subsets with mildly random partitions.}
  \label{4mildrandom}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/6_mild_random.pdf}
  \caption{6x6 subsets with mildly random partitions.}
  \label{6mildrandom}
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/8_mild_random.pdf}
  \caption{8x8 subsets with mildly random partitions.}
  \label{8mildrandom}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/10_mild_random.pdf}
  \caption{10x10 subsets with mildly random partitions.}
  \label{10mildrandom}
\end{subfigure}
\caption{Examples of mildly random partitioning.}
\label{mild_random_partitions}
\end{figure}

Using mildly random partitions as shown in Fig. \ref{mild_random_partitions}, the second portion of the 2D verification study was run from 2x2 to 10x10 subsets in x and y and 1 to 6 angles per quadrant.  Figure \ref{mild_random_verification} shows the results of the time-to-solution estimator (solid line) against Ragusa's code (points) for each test case. The time-to-solution estimator verifies perfectly on mildly random partitions with multiple angles per quadrant. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{../figures/mild_random_verification.pdf}
\caption{A 2D verification suite with mildly random partitions run from 2x2 to 10x10 subsets with each case being run from 1 to 6 angles per quadrant.}
\label{mild_random_verification}
\end{figure}

\subsection{Random Partitions}
%Random partitions
Figure \ref{random_partitions} shows four examples of the random partitioning scheme used for the third part of the verification study. Cut lines in the x dimension go all the way across the domain, but are not necessarily uniformly distributed. The cut lines in y are randomly distributed in each column.This reflects a possible partitioning scheme after the load balancing by dimension algorithm described in Section \ref{sec:lbd} is used.
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/4_random.pdf}
  \caption{4x4 subsets with random partitions.}
  \label{4random}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/6_random.pdf}
  \caption{6x6 subsets with random partitions.}
  \label{6random}
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/8_random.pdf}
  \caption{8x8 subsets with random partitions.}
  \label{8random}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/10_random.pdf}
  \caption{10x10 subsets with random partitions.}
  \label{10random}
\end{subfigure}
\caption{Examples of random partitioning.}
\label{random_partitions}
\end{figure}

Using random partitions as shown in Fig. \ref{random_partitions}, the third portion of the 2D verification study was run from 2x2 to 10x10 subsets in x and y and 1 to 6 angles per quadrant.  Figure \ref{random_verification} shows the results of the time-to-solution estimator (solid line) against Ragusa's code (points) for each test case. The time-to-solution estimator verifies perfectly on random partitions with multiple angles per quadrant. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{../figures/random_verification.pdf}
\caption{A 2D verification suite with random partitions run from 2x2 to 10x10 subsets with each case being run from 1 to 6 angles per quadrant.}
\label{random_verification}
\end{figure}

\subsection{Probable Worst-Case Partitions}
Figure \ref{worst_partitions} shows four examples of the probable worst-case partitioning scheme used for the final part of the verification study. Cut lines in the x dimension go all the way across the domain, and are uniformly distributed. The cut lines in y are distributed on opposing ends of alternating columns.This reflects a possible partitioning scheme after the load balancing by dimension algorithm described in Section \ref{sec:lbd} is used.
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/4_worst.pdf}
  \caption{4x4 subsets with probable worst-case partitions.}
  \label{4worst}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/6_worst.pdf}
  \caption{6x6 subsets with probable worst-case partitions.}
  \label{6worst}
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/8_worst.pdf}
  \caption{8x8 subsets with probable worst-case partitions.}
  \label{8random}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/10_worst.pdf}
  \caption{10x10 subsets with probable worst-case partitions.}
  \label{10random}
\end{subfigure}
\caption{Examples of probable worst-case partitioning.}
\label{worst_partitions}
\end{figure}
Using probable worst-case partitions as shown in Fig. \ref{random_partitions}, the final portion of the 2D verification study was run from 2x2 to 10x10 subsets in x and y and 1 to 6 angles per quadrant.  Figure \ref{worst_verification} shows the results of the time-to-solution estimator (solid line) against Ragusa's code (points) for each test case. The time-to-solution estimator verifies perfectly on probable worst-case partitions with multiple angles per quadrant. 
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{../figures/worst_verification.pdf}
\caption{A 2D verification suite with probable worst-case partitions run from 2x2 to 10x10 subsets with each case being run from 1 to 6 angles per quadrant.}
\label{worst_verification}
\end{figure}

\section{3D Verification}

PDT's performance model was used to verify stage counts for 3D problems with regular grids. Figure \ref{3d_verification} shows PDT's performance model stage counts matching perfectly with the time-to-solution estimator's stage counts for 2\textsuperscript{3} to 10\textsuperscript{3} subsets and from 1 to 6 angles per octant.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{../figures/3d_verification.pdf}
\caption{A 3D verification suite with regular partitions run from 2\textsuperscript{3} to 10\textsuperscript{3} subsets with each case being run from 1 to 6 angles per octant.}
\label{3d_verification}
\end{figure}


\section{PDT performance model vs. TTS}

\section{PDT vs. TTS}
