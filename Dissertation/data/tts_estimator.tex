%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  New template code for TAMU Theses and Dissertations starting Fall 2016.
%
%
%  Author: Sean Zachary Roberson
%  Version 3.17.09
%  Last Updated: 9/21/2017
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           TIME TO SOLUTION ESTIMATOR CHAPTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \setcounter{MaxMatrixCols}{30}
\newcommand{\tcr}[1]{\textcolor{red}{#1}}


\chapter{TIME-TO-SOLUTION ESTIMATOR \label{cha:tts}}
With the introduction of unstructured meshes and imbalanced partitions in PDT, we need an expansion of the performance model to estimate the sweep time.
In addition, before optimization of the partitioning scheme can occur, it is necessary to have an estimation tool that gives the approximate sweep time for a given partitioning scheme.
The time-to-solution estimator serves as the objective function that gets optimized, with the partitions serving as the parameter space. This chapter will detail the time-to-solution estimator, and showcase the results of 2D and 3D verification studies.

The time to solution estimator \jcr{hyphens missing. check throughout. do note that this comment was in my first set of comments. it feels like you did not religiously go through 100\% of those comments} is written in Python 3. Python was chosen as the language because of its powerful graph library, {\tt networkx} \cite{networkx}.
This library gives us a wide variety of graph mathematics and is easy to use.
Before detailing the time-to-solution estimator's methodology, a description of the applicable basic graph theory is necessary.

\section{Graph Theory Applicable to the Time-to-Solution Estimator}

Graph theory is a subset of mathematics that focuses on graphs, or structures containing a set of objects that are connected in a particular manner \cite{graphtheory}.
These objects are called vertices (or nodes), and are connected by edges.
Figure \ref{basic_graph} shows an undirected graph of 4 nodes and 4 edges. The nodes are a mathematical abstraction that can be used to represent a variety of concepts. In this dissertation, they are used to represent the subsets described in Chapter \ref{cha:lb}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{../../figures/undirected_graph.pdf}
\caption{An undirected graph with 4 nodes and 4 edges. }
\label{basic_graph}
\end{figure}
Figure \ref{basic_graph} is referred to as an undirected graph because its edges have no directional information.
If we add directional information to the edges of the graph in Fig. \ref{basic_graph}, it becomes a directed graph, shown in Fig. \ref{directed_graph}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{../../figures/directed_graph.pdf}
\caption{A directed graph with 4 nodes and 4 edges. }
\label{directed_graph}
\end{figure}
In this dissertation, we only use Directed Acyclic Graphs (DAGs), or a graph that has no cycles.  A cycle is defined as a path on a graph that starts and ends at the same vertex. Figure \ref{cycle_example} shows a cycle between nodes 1 and 3.
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{../../figures/cycle_example.pdf}
\caption{A directed graph with a cycle between nodes 1 and 3.}
\label{cycle_example}
\end{figure}
We notice the edge connecting nodes 1 and 3 has a double headed arrow, representing a cycle. Graphs with cycles can be solved for a variety of applications through the use of cycle detection and breaking algorithms, but our partitioning scheme cuts subsets in a fashion that doesn't \jcr{no abbrev. check throughout} allow for cyclical graphs.

Graph edges can be weighted based on the need of the application the graph is being used for. Here, we weight the graph edges to represent the time it takes to solve node A plus the communication time to node B.
Figure \ref{weighted_directed_graph} shows a weighted directed graph.
In the context of the time-to-solution estimator, subset 1 takes 3 seconds to solve and communicate to subset 3.
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{../../figures/weighted_directed_graph.pdf}
\caption{A weighted directed acyclic graph with 4 nodes and 4 weighted edges.}
\label{weighted_directed_graph}
\end{figure}

The time-to-solution estimator uses Johnson's algorithm \cite{intro_to_alg,johnson_nist,johnson_johnson} to find all shortest paths between all pairs in a weighted directed graph.
The weighted shortest path is defined as the path between two nodes that has the smallest weighted sum.
For example, the shortest path between nodes 0 and 3 in Fig. \ref{weighted_directed_graph} is $0\rightarrow 2\rightarrow 3$.

In our application, we use Johnson's algorithm to assist in calculate\jcr{ing} the longest path between two nodes in a DAG (needed in Section \ref{sec:universal}).
The longest path is found by:
\begin{enumerate}
  \item Multiplying all edge weights by -1,
  \item Using Johnson's algorithm to find the shortest (in this case the most negative) paths,
  \item Summing the original weights of this shortest path.
\end{enumerate}
Johnson's algorithm is specifically used in this process because it is capable of finding the shortest path even when edge weights are negative.

%In addition to longest path length calculation, we use an unweighted shortest path algorithm when calculating the depth-of-graph remaining (needed in Section \ref{sec:conflict}).

%\begin{figure}[H]
%\centering
%\includegraphics[scale=0.5]{../../figures/negative_weighted_directed_graph.pdf}
%\caption{The DAG in Fig. \ref{
%\label{negative_weights}
%\end{figure}
Now that the applicable graph theory has been reviewed, we detail the methodology of the time-to-solution estimator.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
The time-to-solution estimator determines the time to sweep across a domain by:
\begin{enumerate}
	\item Given a partitioning scheme, building an adjacency matrix,
	\item Building Directed Acyclic Graphs (DAGs) from the adjacency matrix, one for each quadrant/octant,
	\item Weighting the edges of each graph based on the solve time and communication time of each subset to its neighbors,
     \item Adding and modifying graph weights based on how many anglesets are pipelined,
	\item Modifying the weights of each graph to operate on the universal timescale,
	\item Modifying the weights of each graph to reflect sweep conflicts between octants,
	\item Calculating the time to solution.
\end{enumerate}

\subsection{Building the adjacency matrices}

Before building the graph for each quadrant/octant, an adjacency matrix must be built for the given partitioning scheme.
The adjacency matrix proves\jcr{? provides? another comment that was already present before ... will there be more many more?} connectivity information for each subset to its neighboring subset.
The adjacency building process relies on a major assumption: The z dimension has partitions all the way across the domain, then the x dimension has partitions per plane, then the y dimension has partitions per plane per column.
In future work, these three dimensions can be made interchangeable, but in the present work, this ordering is fixed. \jcr{I fixed the previous sentence. I had put wiggles under it. It looks like most of my comments on the first draft have not been addressed so I am going to stop here. I will just look at the results if they are different but I expect you to go back to the first set of comments are implement them before I read in detail more of this. VERY IMPORTANT. Put a new version before this Saturday, 10pm TX time. Again, subsequent comments will only be about resuts as I expect you to go back and finish work on the text, per initial review.}

Figure \ref{25basematrix} shows a partitioning scheme for 3 subsets each in the x and y dimensions with its corresponding adjacency matrix.

\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\centering
\includegraphics[scale=0.55]{../../figures/boundaries_worst.pdf}
\end{minipage}
\begin{minipage}[c]{0.6\textwidth}
\centering
\scalebox{0.5}{
$\begin{pmatrix}
0&1&0&1&0&0&0&0&0\\
1&0&1&1&0&0&0&0&0\\
0&1&0&1&1&1&0&0&0\\
1&1&1&0&1&0&1&1&1\\
0&0&1&1&0&1&0&0&1\\
0&0&1&0&1&0&0&0&1\\
0&0&0&1&0&0&0&1&0\\
0&0&0&1&0&0&1&0&1\\
0&0&0&1&1&1&0&1&0\\
\end{pmatrix}$}
\end{minipage}
\caption{A 3x3 subset partitioning scheme and its corresponding adjacency matrix.}
\label{25basematrix}
\end{figure}

\subsection{Building the directed acyclic graphs (DAGs)}

The adjacency matrices give us directed connectivity information in order to build our graphs. This process differs slightly from 2D to 3D. Both processes use on networkx's DiGraph function to build the DAGs.

\subsubsection{Building the 2d graphs}
In two dimensions, we build four graphs corresponding to four quadrants.
We define the quadrants in the following manner:
\begin{itemize}
  \item Quadrant 0: $\Omega_x > 0$, $\Omega_y > 0$
  \item Quadrant 1: $\Omega_x > 0$, $\Omega_y < 0$
  \item Quadrant 2: $\Omega_x < 0$, $\Omega_y > 0$
  \item Quadrant 3: $\Omega_x < 0$, $\Omega_y < 0$
\end{itemize}
Figure \ref{quadrant_layout} illustrates this numbering.
\begin{figure}[H]
\centering
\includegraphics{figures/quadrant_layout.pdf}
\caption{The quadrant layout for 2D problems.}
\label{quadrant_layout}
\end{figure}
The initial adjacency matrix we obtain can be immediately used to build the graphs for quadrants 0 and 3 by using networkx's DiGraph function.
We feed the upper triangular portion of the adjacency matrix to DiGraph to get the quadrant 0 graph, and the lower triangular portion to get the quadrant 3 graph.
Utilizing the same partitioning scheme shown in Fig. \ref{25basematrix}, pull the upper triangular and lower triangular portions of the matrix, shown in Fig. \ref{25baseportionmatrices}.
\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\centering
\scalebox{0.6}{
$\begin{pmatrix}
0&1&0&1&0&0&0&0&0\\
0&0&1&1&0&0&0&0&0\\
0&0&0&1&1&1&0&0&0\\
0&0&0&0&1&0&1&1&1\\
0&0&0&0&0&1&0&0&1\\
0&0&0&0&0&0&0&0&1\\
0&0&0&0&0&0&0&1&0\\
0&0&0&0&0&0&0&0&1\\
0&0&0&0&0&0&0&0&0\\
\end{pmatrix}$}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
\centering
\scalebox{0.6}{
$\begin{pmatrix}
0&0&0&0&0&0&0&0&0\\
1&0&0&0&0&0&0&0&0\\
0&1&0&0&0&0&0&0&0\\
1&1&1&0&0&0&0&0&0\\
0&0&1&1&0&0&0&0&0\\
0&0&1&0&1&0&0&0&0\\
0&0&0&1&0&0&0&0&0\\
0&0&0&1&0&0&1&0&0\\
0&0&0&1&1&1&0&1&0\\
\end{pmatrix}$}
\end{minipage}
\caption{The upper triangular (left) and lower triangular (right) portions of the adjacency matrix in Fig. \ref{25basematrix}.}
\label{25baseportionmatrices}
\end{figure}
These matrix portions provide connectivity \textit{and} dependency information for quadrants 0 and 3.
With this information, network's DiGraph function is able to construct the DAGs for these quadrants.
Figure \ref{25_q0q3graphs} shows the the DAGS associated with quadrants 0 and 3, built from the adjacency matrices in Fig. \ref{25baseportionmatrices}.

\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\includegraphics[scale=0.5]{../../figures/9_graph0.pdf}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
\includegraphics[scale=0.5]{../../figures/9_graph3.pdf}
\end{minipage}
\caption{The quadrant 0 DAG (left) and the quadrant 3 DAG(right).}
\label{25_q0q3graphs}
\end{figure}
\jcr{arrows are too small in the figures}\\
Upon inspection of these two DAGs, we see that they show the correct connectivity, dependency, and expected opposing sweep ordering.
Quadrant 0 starts its sweep from subset 0, finishing at subset 8, and quadrant 3 starts its sweep from subset 8, finishing at subset 0.

To obtain the graphs for quadrants 1 and 2, a ``flipped'' version of the adjacency matrix is necessary.
We temporarily renumber the subsets starting from the top left corner, and increasing down each column, as shown in Fig. \ref{25flippedmatrix}.
\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\centering
\includegraphics[scale=0.55]{../../figures/boundaries_worst_flipped.pdf}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
\centering
\scalebox{0.6}{
$\begin{pmatrix}
0&1&0&1&1&1&0&0&0\\
1&0&1&0&0&1&0&0&0\\
0&1&0&0&0&1&0&0&0\\
1&0&0&0&1&0&1&0&0\\
1&0&0&1&0&1&1&0&0\\
1&1&1&0&1&0&1&1&1\\
0&0&0&1&1&1&0&1&0\\
0&0&0&0&0&1&1&0&1\\
0&0&0&0&0&1&0&1&0\\
\end{pmatrix}$}
\end{minipage}
\caption{The flipped subset ordering and corresponding flipped adjacency matrix for the partitioning scheme in Fig. \ref{25basematrix}.}
\label{25flippedmatrix}
\end{figure}
We then get the upper triangular and lower triangular portions of the flipped adjacency matrix to get the connectivity and dependency information for quadrants 1 and 2.
We feed the triangular matrices into networkx's DiGraph function, along with a mapping of the flipped subset ids to the original subset ids (shown in Fig. \ref{25basematrix}). Figure \ref{25_q1q2graphs} shows the DAGs for quadrants 1 and 2.
\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\includegraphics[scale=0.5]{../../figures/9_graph1.pdf}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
\includegraphics[scale=0.5]{../../figures/9_graph2.pdf}
\end{minipage}
\caption{The quadrant 1 DAG (left) and the quadrant 2 DAG(right).}
\label{25_q1q2graphs}
\end{figure}
Upon inspection of these two DAGs, we see that they show the correct connectivity, dependency, and expected opposing sweep ordering.
Quadrant 1 starts its sweep from subset 2, finishing at subset 6, and quadrant 2 starts its sweep from subset 6, finishing at subset 2.

%Weighting the TDGS
\subsection{Weighting the task dependence graphs}

Each graph is weighted to reflect the solve time and communication time of each node to its neighbors. Explicitly, the edge weight between node A and node B represents the solve time of node A added to the time it takes to communicate boundary information from nod A to node B. Equation \ref{weight_function} shows how the weights are calculated:
\begin{equation}
\text{weight} = \text{mcff}\cdot [T_{wu} + N_n\cdot \text{latency}\cdot M_L + T_{\text{comm}}\cdot N_b\cdot A_m\cdot upbc + \cdot N_c\cdot (T_c + A_m\cdot (T_m + T_g))],
\label{weight_function}
\end{equation}
where:
\begin{itemize}
  \item mcff = the Multi-Core Fudge Factor, a corrective factor that accounts for performance drop-off from 1 to 8 cores,
  \item $T_{wu}$ = the time to get into the sweep operator,
  \item $N_n$ = the number of neighbors this node has to communicate to,
  \item latency = the machine specific communication latency,
  \item $M_L$ = the machine specific latency multiplier,
  \item $T_{\text{comm}}$ = the communication time per double,
  \item $N_b$ = the number of boundary cells shared by node A and node B,
  \item $A_m$ = the number of angles node A has to solve prior to communicating,
  \item $upbc$ = the number of boundary unknowns per boundary cell,
  \item $N_c$ = the number of cells in node A,
  \item $T_c$ = the time spent solving cell-specific work,
  \item $T_m$ = the time spent solving angle-specific work,
  \item $T_g$ = the time spent solving group-specific work.
\end{itemize}
The weighting function is based on PDT's performance model\cite{mpadams2015}, which is specific to how PDT solves the transport sweep. The cost function can be modified based on different sweep methodologies if a user desires.

As shown in Eq. \ref{weight_function}, a crucial part of determining the weight of each edge is knowing the number of cells each subset has, and the amount of shared boundary cells with each neighbor. Given a mesh density, the number of cells per subset is given by Eq \ref{cellspersubset}:
\begin{equation}
   \text{cells per subset} = \int_{\tcr{x_i}}^{\tcr{x_{i+1}}} \int_{\tcr{y_j}}^{\tcr{y_{j+1}}} \int_{\tcr{z_k}}^{\tcr{z_{k+1}}} \text{mesh density } dx dy dz,
\label{cellspersubset}
\end{equation}
where the integral bounds represent the cut plane coordinates that form the subset. To estimate the boundary cells to each neighbor, we assume that the mesh within each subset is mostly uniform. Equations \ref{nxy}-\ref{nyz} calculate the boundary cells along each face in 3D:
\begin{align}
n_{xy} &= \Big(\frac{N_c}{V}\Big)^{2/3}\cdot L_x\cdot L_y \label{nxy}, \\
n_{xz} &= \Big(\frac{N_c}{V}\Big)^{2/3}\cdot L_x\cdot L_z \label{nxz}, \\
n_{yz} &= \Big(\frac{N_c}{V}\Big)^{2/3}\cdot L_y\cdot L_z \label{nyz},
\end{align}
where $N_c$ is the number of cells in the subset, $V$ is the subset volume, and $L_d$ is the length of the subset in dimension $d$.
Equations \ref{nx} and \ref{ny} show the 2-dimensional equivalents to \ref{nxy}-\ref{nyz}:
\begin{align}
n_x &= \Big(\frac{N_c}{A}\Big)\cdot L_x, \label{nx} \\
n_y &= \Big(\frac{N_c}{A}\Big)\cdot L_y, \label{ny}
\end{align}
\jcr{are you missing a $1/2$ exponent on ($N_c/A$)????}
where $A$ is the subset area. With this information, we compute all weights in each graph according to Eq. \ref{weight_function}. Once graphs are weighted, we add and modify edge weights for the number of angles pipelined per octant/quadrant.

\subsection{Adding graphs for angular pipelining}

If there are angles to be pipelined, the time-to-solution estimator adds a new set of graphs for each additional angle to be pipelined. For example, if there are two anglesets per octant, this would results in 16 graphs, or 1 graph per octant per angleset. Figure \ref{angular_pipeline} shows the graphs for two anglesets for a quadrant. A ``dummy'' node with a value of -2 is added in order to have an incoming edge for the first subset's node. The value of this incoming edge represents when that angleset starts its sweep.
\begin{figure}[H]
\begin{minipage}[c]{0.5\textwidth}
\centering
\includegraphics[scale=0.6]{../../figures/q0_postpipeline.pdf}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
\centering
\includegraphics[scale=0.6]{../../figures/q4_postpipeline.pdf}
\end{minipage}
\caption{The graphs for the first (left) and second (right) anglesets}
\label{angular_pipeline}
\end{figure}

\subsection{Modifying the weights of each graph to reflect a universal timescale}\label{sec:universal}

Once we have our full set of graphs with angular pipelining accounted for, we set up each graph to reflect a universal timescale, with the goal of knowing when each node in each graph is ready to solve.
For each node in each graph, we:
\begin{enumerate}
  \item Calculate the longest path to the node,
  \item Sum the weights of the edges along the longest path,
  \item Set all incoming edge values to the node to the sum of the longest path.
\end{enumerate}
The incoming edges to each node in each graph now reflect the time at which a node is ready to solve. This universal edge weighting is crucial for detecting and resolving conflicts during the sweep. Figure \ref{universal} shows a simple example of a TDG before and after this weight adjustment.
\begin{figure}[H]
  \begin{minipage}[c]{0.5\textwidth}
    \centering
    \includegraphics[scale=0.5]{../../figures/G_pre_universal.pdf}
  \end{minipage}
  \begin{minipage}[c]{0.5\textwidth}
    \centering
    \includegraphics[scale=0.5]{../../figures/G_universal.pdf}
  \end{minipage}
  \caption{A TDG before (left) and after (right) universal edge weighting is applied.}
   \label{universal}
\end{figure}

\subsection{Modifying the weights of each graph to detect and resolve conflicts}\label{sec:conflict}

At this point in the time-to-solution estimation process, we have a graph per octant/quadrant per angleset, with each graph weighted on a universal time scale. For each node in every graph, the incoming edges to the node represent the time $t$ that it is ready to solve at. The time to solution is best summarized as a ``marching'' process:
\begin{enumerate}
  \item Starting at time $t=0$, find the nodes that are ready to solve or already solving across all graphs.
  \item If at that time, multiple graphs are solving the same node, they are in conflict.
  \item The graph that ``wins'' the conflict does not have its weights modified, while the graph(s) that lose the conflict modify their downstream weights according to how long they are delayed.
  \item Update $t$ to the next interaction across all graphs.
  \item Repeat steps 2, 3, and 4 until all graphs are finished sweeping.
\end{enumerate}

When a conflict is detected, the time-to-solution estimator uses a first-come-first-serve conflict resolution method. The first graph to arrive to a node will begin solving it, and the remaining graphs that arrive while it is being solved will incur a delay. The delay is reflected in the remaining graphs by adding the delay as a weight to the applicable edge and all downstream edges in the losing graphs.

If two or more graphs arrive to a node at the same time, the octant with the greater remaining depth-of-graph (simply, more work remaining), wins. In the case of a tie in the depth-of-graph, the graph with the priority direction wins according to the following rules:
\begin{enumerate}
    \item The graph with $\Omega_x > 0$ wins,
	\item If multiple graphs have $\Omega_x > 0$, then the task with $\Omega_y > 0$ wins,
	\item If multiple graphs have $\Omega_y > 0$, then the task with $\Omega_z > 0$ wins.
\end{enumerate}
The delay is once again added to the applicable edge's weight and all downstream edges' weights.

\subsection{Estimating the final time-to-solution}

Once all graphs have had their weights modified for conflicts, the graphs now reflect a schedule. The incoming edges to each node in each graph represent what time they are ready to solve. The final weight (the outgoing edge of the final subset) in each graph represents the time it takes for that graph to sweep across its domain. The maximum final weight across all graphs represents the estimate for the time-to-solution for the problem.

\section{2D Verification}

A theoretical study in 2D was run to verify the time-to-solution estimator for 2D partitioning schemes with perfectly balanced partitions. The test problems were verified against a code written by Jean Ragusa that uses a depth-of-graph with an octant priority tie breaker scheduler in two dimensions. The verification study consists of the following problems:
\begin{enumerate}
	\item 2x2 to 10x10 subsets in x and y with regular partitions and 1 to 6 anglesets per quadrant.
	\item 2x2 to 10x10 subsets in x and y with ``mildly random'' partitions and 1 to 6 anglesets per quadrant.
	\item  2x2 to 10x10 subsets in x and y with ``random'' partitions and 1 to 6 anglesets per quadrant.
	\item  2x2 to 10x10 subsets in x and y with probable worst-case partitions and 1 to 6 anglesets per quadrant.
\end{enumerate}

"Mildly random" partitions keep the cut lines uniformly distributed in x, while the y cut lines vary slightly around the uniformly distributed cut lines of the regular partitions. Figure \ref{mild_random_partitions} shows examples of this partitioning style. "Random" partitions possesses no such limitations on either set of cut lines, as shown by Fig. \ref{random_partitions}. The ``mildly random'' and ``random'' partition styles mimic likely partitioning schemes we can expect from load-balancing-by-dimension.

Figures \ref{regular_partitions}, \ref{mild_random_partitions}, \ref{random_partitions}, \ref{worst_partitions} show the four partitioning schemes and Figs. \ref{regular_verification}, \ref{mild_random_verification}, \ref{random_verification}, \ref{worst_verification} show the results of the verification study for each partitioning scheme. In the results, a stage is defined as the time it takes to solve all cells in a subset for an angle.

\subsection{Regular Partitions}

Figure \ref{regular_partitions} shows four examples of the regular partitioning scheme used for the first part of the verification study. Cut lines in both dimensions go all the way across the domain. This reflects the partitioning scheme after the original load balancing algorithm described in Section \ref{sec:og_lb} is used.

%Regular partitions
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/4_regular.pdf}
  \caption{4x4 subsets with regular partitions.}
  \label{4regular}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/6_regular.pdf}
  \caption{6x6 subsets with regular partitions.}
  \label{6regular}
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/8_regular.pdf}
  \caption{8x8 subsets with regular partitions.}
  \label{8regular}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/10_regular.pdf}
  \caption{10x10 subsets with regular partitions.}
  \label{10regular}
\end{subfigure}
\caption{Examples of regular partitioning.}
\label{regular_partitions}
\end{figure}

Using regular partitions as shown in Fig. \ref{regular_partitions}, the first portion of the 2D verification study was run from 2x2 to 10x10 subsets in x and y and 1 to 6 angles per quadrant.  Figure \ref{regular_verification} shows the results of the time-to-solution estimator (solid line) against Ragusa's code (points) for each test case. The time-to-solution estimator is in perfect agreement for regular partitions with multiple angles per quadrant.

%Verification plots.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{../figures/regular_verification.pdf}
\caption{A 2D verification suite with regular partitions run from 2x2 to 10x10 subsets with each case being run from 1 to 6 anglesets per quadrant.}
\label{regular_verification}
\end{figure}

\subsection{``Mildly Random'' Partitions}
Figure \ref{mild_random_partitions} shows four examples of the``mildly random'' partitioning scheme used for the second part of the verification study. Cut lines in the x dimension go all the way across the domain, and are uniformly distributed. This reflects a possible partitioning scheme after the load balancing by dimension algorithm described in Section \ref{sec:lbd} is used.

%Mild random partitions
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/4_mild_random.pdf}
  \caption{4x4 subsets with``mildly random'' partitions.}
  \label{4mildrandom}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/6_mild_random.pdf}
  \caption{6x6 subsets with``mildly random'' partitions.}
  \label{6mildrandom}
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/8_mild_random.pdf}
  \caption{8x8 subsets with``mildly random'' partitions.}
  \label{8mildrandom}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/10_mild_random.pdf}
  \caption{10x10 subsets with``mildly random'' partitions.}
  \label{10mildrandom}
\end{subfigure}
\caption{Examples of``mildly random'' partitioning.}
\label{mild_random_partitions}
\end{figure}

Using``mildly random'' partitions as shown in Fig. \ref{mild_random_partitions}, the second portion of the 2D verification study was run from 2x2 to 10x10 subsets in x and y and 1 to 6 angles per quadrant.  Figure \ref{mild_random_verification} shows the results of the time-to-solution estimator (solid line) against Ragusa's code (points) for each test case. The time-to-solution estimator is in perfect agreement for``mildly random'' partitions with multiple angles per quadrant.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{../figures/mild_random_verification.pdf}
\caption{A 2D verification suite with``mildly random'' partitions run from 2x2 to 10x10 subsets with each case being run from 1 to 6 anglesets per quadrant.}
\label{mild_random_verification}
\end{figure}

\subsection{Random Partitions}
%Random partitions
Figure \ref{random_partitions} shows four examples of the``random''partitioning scheme used for the third part of the verification study. Cut lines in the x dimension go all the way across the domain, but are not necessarily uniformly distributed. The cut lines in y are randomly distributed in each column.This reflects a possible partitioning scheme after the load balancing by dimension algorithm described in Section \ref{sec:lbd} is used.
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/4_random.pdf}
  \caption{4x4 subsets with``random''partitions.}
  \label{4random}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/6_random.pdf}
  \caption{6x6 subsets with``random''partitions.}
  \label{6random}
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/8_random.pdf}
  \caption{8x8 subsets with``random''partitions.}
  \label{8random}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/10_random.pdf}
  \caption{10x10 subsets with``random''partitions.}
  \label{10random}
\end{subfigure}
\caption{Examples of``random''partitioning.}
\label{random_partitions}
\end{figure}

Using``random''partitions as shown in Fig. \ref{random_partitions}, the third portion of the 2D verification study was run from 2x2 to 10x10 subsets in x and y and 1 to 6 angles per quadrant.  Figure \ref{random_verification} shows the results of the time-to-solution estimator (solid line) against Ragusa's code (points) for each test case. The time-to-solution estimator is in perfect agreement for``random''partitions with multiple angles per quadrant.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{../figures/random_verification.pdf}
\caption{A 2D verification suite with``random'' partitions run from 2x2 to 10x10 subsets with each case being run from 1 to 6 anglesets per quadrant.}
\label{random_verification}
\end{figure}

\subsection{Probable Worst-Case Partitions}
Figure \ref{worst_partitions} shows four examples of the probable worst-case partitioning scheme used for the final part of the verification study. Cut lines in the x dimension go all the way across the domain, and are uniformly distributed. The cut lines in y are distributed on opposing ends of alternating columns.This reflects a possible partitioning scheme after the load balancing by dimension algorithm described in Section \ref{sec:lbd} is used.
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/4_worst.pdf}
  \caption{4x4 subsets with probable worst-case partitions.}
  \label{4worst}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/6_worst.pdf}
  \caption{6x6 subsets with probable worst-case partitions.}
  \label{6worst}
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/8_worst.pdf}
  \caption{8x8 subsets with probable worst-case partitions.}
  \label{8random}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{../cut_line_files/10_worst.pdf}
  \caption{10x10 subsets with probable worst-case partitions.}
  \label{10random}
\end{subfigure}
\caption{Examples of probable worst-case partitioning.}
\label{worst_partitions}
\end{figure}
Using probable worst-case partitions as shown in Fig. \ref{worst_partitions}, the final portion of the 2D verification study was run from 2x2 to 10x10 subsets in x and y and 1 to 6 angles per quadrant.  Figure \ref{worst_verification} shows the results of the time-to-solution estimator (solid line) against Ragusa's code (points) for each test case. The time-to-solution estimator is in perfect agreement for probable worst-case partitions with multiple angles per quadrant.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{../figures/worst_verification.pdf}
\caption{A 2D verification suite with probable worst-case partitions run from 2x2 to 10x10 subsets with each case being run from 1 to 6 angles per quadrant.}
\label{worst_verification}
\end{figure}

\section{3D Verification}
\jcr{ok. I am actually resuming from here because everything below is actually NEW!}

PDT's performance model was used to verify stage counts for 3D problems with regular grids. \jcr{as before, I have any issue with your choice of tense.}
Figure \ref{3d_verification} shows PDT's performance model stage counts matching perfectly with the time-to-solution estimator's stage counts for 2\textsuperscript{3} to 10\textsuperscript{3} subsets and from 1 to 6 angles per octant.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{../figures/3d_verification.pdf}
\caption{A 3D verification suite with regular partitions run from 2\textsuperscript{3} to 10\textsuperscript{3} subsets with each case being run from 1 to 6 angles per octant.}
\label{3d_verification}
\end{figure}


\section{PDT's Performance Model vs. Time-to-Solution Estimator}

PDT has run a scaling suite out to 90,112 cores on the Quartz supercomputer \cite{quartz} at Lawrence Livermore National Lab (LLNL).
A smaller scaling suite, tabulated in Table \ref{scaling_suite} was the first benchmark case run for the time-to-solution estimator.
The suite was run with 1 energy group, 80 directions, $A_m = 10$, and $A_z = 1$.
The following machine parameters were generated and used for the 1 group suite:
\begin{itemize}
  \item $T_c = 2683.769$ ns
  \item $T_m = 111.972$ ns
  \item $T_g = 559.127$ ns
  \item $T_\text{byte} = 4.47$ ns
  \item $\text{latency }= 4110$ ns
  \item $M_L = 2.5$
  \item $T_{wu} = 5779.929$ ns
  \item mcff = 1.32
\end{itemize}

\begin{table}[H]
  \centering
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \textbf{Cores} & 1 & 8 & 64 & 512 & 1,204 & 2,048 & 4,096 & 8,192 & 16,384 \\ \hline
    \textbf{Nx} & 16 & 32 & 64 & 128 & 128 & 256 & 256 & 256 & 512 \\ \hline
    \textbf{Ny} & 16 & 32 & 64 & 128 & 128 & 128 & 128 & 256 & 256 \\ \hline
    \textbf{Nz} & 16 & 32 & 64 & 128 & 256 & 256 & 256 & 512 & 512 \\ \hline
    \textbf{Px} & 1  & 2  & 8  & 16  & 32  & 32  & 64  & 64  & 128 \\ \hline
    \textbf{Py} & 1  & 2  & 4  & 16  & 16  & 32  & 32  & 64  & 64  \\ \hline
    \textbf{Pz} & 1  & 2  & 2  & 2   & 2   & 2   & 2   & 2   & 2   \\
  \end{tabular}
  \caption{The scaling suite parameters ran with 1 energy group, 80 directions, and $A_m = 10$.}
  \label{scaling_suite}
\end{table}
\jcr{what are the $A_x$, $A_y$ and values?}

Figure \ref{scaling_stagecount} shows the stage counts of the scaling suite for PDT's performance model and the time-to-solution estimator are in perfect agreement.
Figures \ref{weak_scaling_tts_sweep} and \ref{weak_scaling_tts} show the time per sweep and parallel efficiency for PDT, the PDT performance model, and the time-to-solution estimator \soutr{, and}\jcr{.} Table \ref{scaling_percent_diff} tabulates the percent differences for \jcr{(1)} PDT and the performance model, \jcr{(2)} the performance model\jcr{,} and \jcr{(3)} the time-to-solution estimator, and PDT and the time-to-solution estimator. \jcr{see my mods. check if you need to do this elsewhere!}

It is noticeable that the time-to-solution estimator consistently returns a smaller sweep time value than PDT's performance model. This is due to\jcr{you have no proof of that} the assumption from PDT's performance model that each processors at each stage communicate to the same amount of neighbors (three neighbors in 3D, 2 in 2D). In reality, this is not the case, as processors in the corners of the domain will communicate to less\jcr{fewer} neighbors for certain directions, and the final processor for a direction does not communicate to any neighbors (as it has no neighbors).
Even given these differences, the time-to-solution estimator is consistently within 4\% of the performance model.

The performance model's slightly better accuracy in predicting PDT's sweep time is likely due to processor noise and slight variations in latency on Quartz driving up PDT's sweep time. This masks the performance model's overestimation of the sweep time.


\begin{figure}[ht]
\centering
\includegraphics[scale=0.8]{../../figures/scaling_stagecount.pdf}
\caption{The stage counts of PDT's performance model and the time-to-solution estimator for the scaling suite in Table \ref{scaling_suite}.}
\label{scaling_stagecount}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.8]{../../figures/scaling_tts_sweep_times.pdf}
\caption{The time per sweep of PDT, the PDT performance model, and the time-to-solution estimator.}
\label{weak_scaling_tts_sweep}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.8]{../../figures/scaling_tts.pdf}
\caption{The parallel efficiency relative to 8 cores of PDT, the PDT performance model, and the time-to-solution estimator.}
\label{weak_scaling_tts}
\end{figure}
\jcr{fig \ref{weak_scaling_tts} is not discussed in the text}
\jcr{likewise for table\ref{scaling_percent_diff}}

\begin{table}[ht]
\centering
\caption{The percent difference between PDT and its performance model, PDT's performance model and the time-to-solution estimator, and PDT and the time-to-solution estimator.}
\label{scaling_percent_diff}
\begin{tabular}{c|c|c|c}
\textbf{Cores} & \textbf{PDT v. Perf.} & \textbf{Perf. v. TTS} & \textbf{PDT v. TTS} \\ \hline
1&8.82\%&0.68\%&9.44\% \\ \hline
8&6.67\%&1.86\%&8.4\% \\ \hline
64&2.22\%&2.04\%&4.22\% \\ \hline
512&8.0\%&1.52\%&9.4\% \\ \hline
1024&2.0\%&3.67\%&5.6\% \\ \hline
2048&2.0\%&2.21\%&4.17\% \\ \hline
4096&3.77\%&3.24\%&6.89\% \\ \hline
8192&8.33\%&3.8\%&11.82\% \\ \hline
16384&9.68\%&2.7\%&12.11\%
\end{tabular}
\end{table}

\section{PDT vs. Time-to-Solution Estimator for Unstructured Meshes}

The motivation to develop the time-to-solution estimator was born out of the desire to predict sweep time for unstructured meshes.
To test how the time-to-solutions estimator performs on unstructured problems, we use the meshes from our load balancing parametric study (Fig. \ref{partitioning_example}) and the level Level 2 experiment (Fig. \ref{level2_nocut}).\jcr{this is later. I think if you do some level-2 earlier, as I recommended somewhere, it will be smoother when you reach this paragraph}

\jcr{each figure and table needs to be discussed in the text}\\

jcr{I do not understand the LBD meshes for level-2. the x-cuts go all the way across. fine. however, this is actually not true. so x-cut do not go all the way, around x=18 and y=53 in the supposedly evenly partitioned case. what's going on with the level-2 mesh ???}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.75]{../../figures/spiderweb_reg_pdtvtts.pdf}
\caption{The sweep times of the time-to-solution estimator and PDT for the mesh in Fig. \ref{partitioning_example} for 2 to 10 subsets in each dimension.}
\label{comp_reg_spiderweb}
\end{figure}

\begin{table}[ht]
\centering
\caption{The percent difference in sweep times between the time-to-solution estimator and PDT for the sweep times shown in Fig. \ref{comp_reg_spiderweb}.}
\begin{tabular}{c|c}
\textbf{$\sqrt{\text{Num Subsets}}$} & \bf PDT vs. TTS \\ \hline
2&24.13\%\\ \hline
3&11.84\%\\ \hline
4&1.46\%\\ \hline
5&10.08\%\\ \hline
6&2.76\%\\ \hline
7&7.76\%\\ \hline
8&5.12\%\\ \hline
9&3.68\%\\ \hline
10&0.93\%
\end{tabular}
\end{table}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.3]{../../figures/level2_nocut.png}
\caption{The mesh for the Level 2 experiment.}
\label{level2_nocut}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.3]{../../figures/level2_42x13.png}
\caption{The Level 2 experiment mesh evenly partitioned into 42 subsets in x and 13 subsets in y. $f = 32.616$.}
\label{level2_42x13}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.3]{../../figures/level2_42x13_balanced.png}
\caption{The Level 2 experiment mesh partitioned and load balanced with 42 subsets in x and 13 subsets in y. $f = 2.386$.}
\label{level2_42x13_balanced}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.15]{../../figures/level2_nocut_zoom.png}
\caption{The detector region of the Level 2 experiment mesh.}
\label{level2_nocut_zoom}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.15]{../../figures/level2_42x13_zoom.png}
\caption{The detector region of the Level 2 experiment when partitioned into 42 subsets in x and 13 subsets in y.}
\label{level2_42x13_zoom}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.15]{../../figures/level2_42x13_balanced_zoom.png}
\caption{The detector region of the Level 2 experiment when partitioned and balanced with 42 subsets in x and 13 subsets in y.}
\label{level2_42x13_balanced_zoom}
\end{figure}
